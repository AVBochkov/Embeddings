# Embeddings
Constructing Transformer language models in which the token embedding layer is entirely frozen and derived not from data-driven optimization, but from the visual structure of Unicode glyphs and token-level n-gram renderings
